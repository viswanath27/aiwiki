{% extends "layout.html" %}

{% block content %}
<div>
    <h1>DEEP LEARNING</h1>
    <p>Deep learning is the concept which is majorly inherited from Brian functionality, which has lots of Neurons
and connection between them. Single Neuron by itself does not make much but when lots of them combined
will be make it intelligent.</p>
<p>A sample neuron is visualized as below.</p>
<img src="{{url_for('static', filename='neuron.png')}}" alt="" align="middle" width="400" height="200"><br>
<p>Deep learing consists of majorly artificial neural networks that are modelled on similar networks present in
the human brian. As travesers the layers each layer processes an aspect of data and removes the outliers,
spots, familiar entities and produce the final output.</p>
<br>
<img src="{{url_for('static', filename='ANN.jpeg')}}" alt="" align="middle" width="300" height="200"><br>
<br>
<ul>
<li>1) Input Layer : Does nothing but taking the input and passing it to the next layers.</li>
<li>2) Output Layer : this is predicted feature, will depend on how many features we want to predict.</li>
<li>3) Hidden Layers : These layers apply transformations to the input before passing to its next layers.</li>
</ul>
<p>As network is getting trained, wieghts get trained to make model more predictive.
Weights are generally initialized to some random numbers and they will be in range of 0 and 1.</p>
<br>

<h2>Feedforward Deep Networks</h2>
<p>Feed forward supervised learning is also called as Multi-Layer Perceptron(MLP). It has one hidden layer and
each neuron is connected to other neuron. This network processes the input upward activating neurons as it goes to finally produce an output value.</p>

<img src="{{url_for('static', filename='FNN.png')}}" alt="" align="middle" width="400" height="300"><br>

<h2>Activiation Function</h2>
An activation function is a mapping of summed weighted input to the output of the neuron. It is called
an activation/transfer function because it governs the inception at which the neuron is activated and the
strength of the output signal.<br><br>
<img src="http://latex.codecogs.com/gif.latex?Y = \sum(weight*input) + bias" border="0"/><br><br>

<p>We have many activation functions out of which most used are relu, tanh, softplus
Below is the cheetsheet.</p><br><br>

<img src="{{url_for('static', filename='activation_functions.png')}}" alt="" align="middle" width="800" height="600"><br><br>

<h2>Back propogation</h2>
<p>The predicted value is compared to the expected output and an error is calcualted using a function. This
error is propogated back within the whole network (one layer at time) and weigths are updated according to
the value that they contributed to the error. This is called as Back-propogation algorithm. This process is
repeated for all of the examples in your training dataset. One round of updating the network for the entire
training dataset is called as an epoch. A network may be trained for tens, hundreds or many thousands of
epochs.</p>

<img src="{{url_for('static', filename='bp.png')}}" alt="" align="middle" width="500" height="300"><br>

<h2>Cost Function and Gradient Descent</h2>
<p>The cost function is the measure of "how good" a neural network did for its given training input and the
expected output. It may also depend on weights and bias.
A cost function is single-valued, not a vector because it rates how well the neural network performed as a
whole. Using the Gradient Descent optimization algorithm, the weights are updated incrementally after each
epoch.</p>
Compatible Cost Function
Sum of squared errors (SSE)
<p>
$$J(w) = 1/2 ((target)^i - (output)^i)^2$$
</p>

<h2> Usecase</h2>
<h2> Example</h2>
<h2> Code</h2>
<h2> Links </h2>
<p>Following section has all the links from which the above content is been understood and presnted.</p>
<ul>
<li><a href="https://towardsdatascience.com/deep-learning-with-python-703e26853820">1. https://towardsdatascience.com/deep-learning-with-python-703e26853820</a></li>
</ul>
<h2> Conclusion </h2>
</div>
{% endblock content %}