{% extends "layout.html" %}
{% block content %}
<div>
<h1> Linear Discriminant Analysis </h1>
<p>This section explains the Linear Discriminant Analysis of the machine learning and its uses and its examples
Little bit about the problem statement of where Linear Discriminant Analysis is useful.</p>
<p>Suppose there is this new cancer drug
	<ul>
<li>1) It works great for some people.</li>
<li>2) But for some people it makes it worse.</li>
</ul>
</p>
<p>How do we decide who to give the drug to ?
We want to give it to people whom this will help but now harm. May be gene expression can help us decide
here.If we plot the gene expression on the number line and see, there can be segregation of dots which are
all one side (people for which drug works) rest of dots allligned at other side (people for which drug does
not work for). This is basically low transcription of gene X and druf is not working for the people who have
high transcription of gene X. There can be an overlap in middle which is not a clear cuto of who to give
the drug to. In summary Gene X does an okay job to tell us who should be drug given vs whom it should
not be given. Can we improve this situation, what if we can use more than one Gene to make a decision.
There could be Gene X and Gene Y plotted in 2D space where it shows who gets the drug and who does
not get. Here there can be a line in 2D space where below which drug works and the other side drug does
not work. This is more clear and better than just using one Gene. How about using 3 Genes. Here we can
have a plane to seperate the boundary between where the drug works and where it will not work. What if
we need 4 or more Genes to seperate two categories. We cannot draw such dimension graph which is more
than 3D. This is similar to PCA problem. PCA reduces dimensions by focusing on the genes with the most
variation. This can be useful to plotting data with a lot of dimensions onto a simple XY plot. However in
we are not interested in the genes with most variation. But we are interested in maximizing the separability
between the two groups so that we can make a best decision.</p>
<p>LDA is like PCA, but it focuses on maximizing the separability among known categories. This thought
process should understood so deeply such that we should be able to gure out the same meaning from the
mathematics to understand what brings that dierence.</p>
<p>We can start with super simple example of reducing the 2D graph to 1D graph.
one bad way to do it to ignore the Gene Y and project all th 2D to X axis.</p>
<p>LDA uses the information from both the axis to create a new axis. This other axis is used to project the
information on to it, in a way to maximize the seperation of the two categories.In summary LDA creates a
new axis and projects the data on to it such a way that it maximizes the dierence of these two categories.
Lets look at how LDA creates new axis.</p>
<p>[Criteria 1]. The new axis is created according to new criteria that are considered simultaneously. The
criteria is once the data is projected on to a new axis, we want to Maximize the distance between two means.</p>
<p>[Criteria 2]. Minimize the variation (which LDA calls "Scatter" and is represented by s2) within each
category.</p>
<p>If we have to consider these two criteria simultaneously, we have the ratio of these both criteria as shown below.</p>
</div>
{% endblock content %}