{% extends "layout.html" %}

{% block content %}
<div>
    <h1>MODEL SELECTION</h1>

This section explains the Classication and Regression Trees of the machine learning and its uses and its
examples
<h2>CROSS VALIDATION - EVALUATING ESTIMATOR PERFORMANCE</h2>
Validating the model with the same data which is used for training is atmost blunder as this would always
lead to the correct result and this kind of models will fail to predict when new data is presented. This
situation is called overtting.
<h3>Code</h3>
Code for this function is provided in below link:
<h3>Example</h3>
Example usage of the Logistic Regression Model


<h2>Tuning the Hyper Parameters of an estimator</h2>
<p>Hyper Parameters are parameters that are not directly learnt within estimators. In scikit-learn they are
passed as arguments to the constructor of the estimator calsses.These are model values you set before you
train on your dataset. ML model is a formula where there are lot of paramters has to be learnt from data.
But there are some of the paramters which cannot be learnt from data. These higher level properties as
Hyper parameters (Ex: this could be number of trees in Random forest or number of layer in Neural network
or the learning rate for the logistic regression). This is sometimes a process of trial and error, its not intutive.
What if we can make this parameters learn the optimal values themselves.</p><br>
<p>Suppose there is Support Vector Machine to decide the emotion of the tweets. 0 for Negative and 1 for Posi-
tive, there is a dataset which those labels already in. This a classic binary classication problem. Supposing
the we create a Support Vector machine which can learn from this data and classify a new tweet as Negative
or Positive. This is called Sentiment Analysis. This is a very popular task in NLP. Once we plot the data
in 2-D space there will boundary line which separates the positve and negative emotions. SVM can help us
nd this decision barrier. SVM is non-linear or it will basically use the Kernel trick. With Kernel the data
gets plotted to a new higher dimensional space. This is called as feature space. This is doing a nonlinear
transformation using a Kernel (or similarity function) and using a linear model in feature space. Kernel or
similarity function is called Radial Basis Function. There are two hyper parameters that govern how our
line will be drawn. These two parameters (C and Gamma) depend on each other in unknown ways. We
cannot just optimize one at a time and just add the result. Also we cannot try every single possible value of
parameters. We can have a variable to store our models accuracy for each set of the hyper parameters. We
iterate the set of values for each of the hyper parameter with the other complete set. We train the model
and score the model and compare the score with the best score. This will run and provides us the optimal
set of Hyper Parameters. This is also called Grid search technique. But if there are more hyper parameters
this process sclaes very poorly (this issue is also called as "Curse of Dimensionality").Instead of complete
search we can go ahead with Random set of parameters and see if we can get best results.</p><br>
<p>Following are some of the methods used</p>
<ul>
<li>1) Manual Search</li>
<li>2) Grid Search</li>
<li>3) Random Search</li>
</ul><br>
<p>Instead of above there is other techinque called as Bayesian Optimization. Bayes theorem is way to deter-
mine conditional probability. This will update the existing prediction given a new evidence. This more line
Frequentist (Objective) (vs) Bayesian (Subjective) groups</p>
<p><b>Bayesian</b></p>
<p>This is more probablistic, it focuses on probability of the hypothesis given the data. That means Data is
xed and hypothesis is random.</p>
<p><b>Frequentist</b></p>
<p>These guys focuses on Probability of the data given the Hypothesis. Data is random and hypothesis is xed.
Hyper Parameter tuning, Bayesian takes advantage of learning better. We pick some prior belief of how our
Hyper params will behave and than search the parameter space by enforcing and updating our prior belief
based on our ongoing measurement. So the tradeo between the exploration ensuring that we visited the
relevant coordinates of our space and exploitation once we found the promising space and nding an optimal
value in it is handled in a more intelligent way.</p><br>
<ul>
	<li>1) Baysian Evaluation uses previously evaluated points to compute a postierior expectation of what the loss
function looks like.</li>
<li>2) Than it samples the loss at a new point that maximizes some utility of the expectation of function.</li>
<li>3) That Utility tells us which region of the domain of function are best to sample from.</li>
<p>This two step process is repeated until convergence. For the prior distribution we can assume that f can be
described by a Gaussian process.</p>
</ul><br>

<p>While Gaussian distribution is specied by</p>
<ul>
	<li>a) mean</li>
	<li>b) variance</li>
</ul>
<p>but Gaussian Process is dened by</p>
<ul>
	<li>a) Mean function</li>
	<li>b) Variance function.</li>
</ul>
<p>The way we nd the best point to sample f next from is to pick the point that maximizes an acquisition
function. This is a function of the posterior distribution over function that describes the utility for all values
of the hyper params. The values that have he highest utility will be the values you compute the loss for
next. We can use the popular expected improvement function. Where x is the current optimal set of hyper
parameters. By maximizing this it will give us the point that improves on the function most so given the
observed values funciont of x we update the posterior expectation of function using the GP model. Then
we nd the new x that maximizes the aquistion function the expected improvement and nally compute the
value of f for the new x.</p><br>
<img src="http://latex.codecogs.com/gif.latex?$EI(X) = E[max\{0,f(X) -f(\hat{X})\}]$" border="0"/><br></p>
<h3>Code</h3>
Code for this function is provided in below link:
<h3>Example</h3>
Example usage of the Logistic Regression Model
5
<h2>Model Evaluation: Quatifying the quality of predictions</h2>
<h3>Code</h3>
Code for this function is provided in below link:
<h3>Example</h3>
Example usage of the Logistic Regression Model

<h2>Model Persistance</h2>
<h3>Code</h3>
Code for this function is provided in below link:
<h3>Example</h3>
Example usage of the Logistic Regression Model

<h2>Validation Curves: Plotting scores to evaluate models</h2>
<h3>Code</h3>
Code for this function is provided in below link:
<h3>Example</h3>
Example usage of the Logistic Regression Model

</div>
{% endblock content %}