{% extends "layout.html" %}

{% block content %}
<div>
    <h1>CLUSTERING </h1>
    <p>Clustering is a concept of organizing the data on a line, 2 D map or graph or may be 3 D. This would help
us to know how the data is organized and what we can derive about the data.Information about the all the
algorithms which are mentioned below. One of the sample use case of each of them. Identify what best
ts the use-case and why. What are the limitations of each of the method. How to improve the algorithm
to make it better an computationally eective. Also see if there are any ways in which nature applies such
algorithms and if so how will it do. Below are the list of the Clustering Algorithms. Also always understand
the mathematical representation of the algorithms which are simple and easy persecive, so that complex
algorithm can be easily understood.</p>
tf.estimator
tf.estimator.BaselineClassifier
tf.estimator.BaselineEstimator
tf.estimator.BaselineRegressor
tf.estimator.BestExporter
tf.estimator.BinaryClassHead
tf.estimator.BoostedTreesClassifier
tf.estimator.BoostedTreesEstimator
tf.estimator.BoostedTreesRegressor
tf.estimator.CheckpointSaverHook
tf.estimator.CheckpointSaverListener
tf.estimator.DNNClassifier
tf.estimator.DNNEstimator
tf.estimator.DNNLinearCombinedClassifier
tf.estimator.DNNLinearCombinedEstimator
tf.estimator.DNNLinearCombinedRegressor
tf.estimator.DNNRegressor
tf.estimator.Estimator
tf.estimator.EstimatorSpec
tf.estimator.EvalSpec
tf.estimator.Exporter
tf.estimator.FeedFnHook
tf.estimator.FinalExporter
tf.estimator.FinalOpsHook
tf.estimator.GlobalStepWaiterHook
tf.estimator.Head
tf.estimator.LatestExporter
tf.estimator.LinearClassifier
tf.estimator.LinearEstimator
tf.estimator.LinearRegressor
tf.estimator.LoggingTensorHook
tf.estimator.LogisticRegressionHead
tf.estimator.ModeKeys
tf.estimator.MultiClassHead
tf.estimator.MultiHead
tf.estimator.MultiLabelHead
tf.estimator.NanLossDuringTrainingError
tf.estimator.NanTensorHook
tf.estimator.PoissonRegressionHead
tf.estimator.ProfilerHook
tf.estimator.RegressionHead
tf.estimator.RunConfig
tf.estimator.SecondOrStepTimer
tf.estimator.SessionRunArgs
tf.estimator.SessionRunContext
tf.estimator.SessionRunHook
tf.estimator.SessionRunValues
tf.estimator.StepCounterHook
tf.estimator.StopAtStepHook
tf.estimator.SummarySaverHook
tf.estimator.TrainSpec
tf.estimator.VocabInfo
tf.estimator.WarmStartSettings
tf.estimator.add_metrics
tf.estimator.classifier_parse_example_spec
tf.estimator.experimental
tf.estimator.experimental.InMemoryEvaluatorHook
tf.estimator.experimental.LinearSDCA
tf.estimator.experimental.RNNClassifier
tf.estimator.experimental.RNNEstimator
tf.estimator.experimental.build_raw_supervised_input_receiver_fn
tf.estimator.experimental.call_logit_fn
tf.estimator.experimental.make_early_stopping_hook
tf.estimator.experimental.make_stop_at_checkpoint_step_hook
tf.estimator.experimental.stop_if_higher_hook
tf.estimator.experimental.stop_if_lower_hook
tf.estimator.experimental.stop_if_no_decrease_hook
tf.estimator.experimental.stop_if_no_increase_hook
tf.estimator.export
tf.estimator.export.ClassificationOutput
tf.estimator.export.ExportOutput
tf.estimator.export.PredictOutput
tf.estimator.export.RegressionOutput
tf.estimator.export.ServingInputReceiver
tf.estimator.export.TensorServingInputReceiver
tf.estimator.export.build_parsing_serving_input_receiver_fn
tf.estimator.export.build_raw_serving_input_receiver_fn
tf.estimator.regressor_parse_example_spec
tf.estimator.train_and_evaluate

	<ul>
<li>1) K- Means</li>
<li>2) Anity Propogation</li>
<li>3) Mean-shift</li>
<li>4) Spectral Clustering</li>
<li>5) Ward Hierarchial Clustering</li>
<li>6) Agglomerative clustering</li>
<li>7) DBSCAN</li>
<li>8) Gaussian Mixtures</li>
<li>9) Brich</li>
</ul>    
<h2>K-MEANS</h2>
<p>In K-Mean algorithm we understand how to pick the best value for K. Once we plot a data based on distribu-
tion we get to its allignment we can see that data is alligned in clusters. But in how can we make computer
to identify these clusters. Following are the steps which are present in K-means cluster.</p>
<ul>
	<li><b>[STEP 1]:</b> Select the number of the clusters you want to end up after clustering. This is the K in K-Means
cluster (like example if you want 3 clusters, k will be 3). There also a better way to select K</li><br>
	<li><b>[STEP 2]:</b> Randomly select 3 distant Data points.These will be the initial clusters.</li><br>
	<li><b>[STEP 3]:</b> Measure the distance between the rst point and the 3 initial clusters.</li><br>
	<li><b>[STEP 4]:</b> Assign the rst point to the nearest cluster.</li><br>
	<li><b>[STEP 5]:</b> Repeat the same to the second point and assign the cluster which is near to it.</li><br>
	<li><b>[STEP 6]:</b> After this step we should calculate the mean of each cluster. That would be the average of all the
groups of the points in cluster.</li><br>
	<li><b>[STEP 7]:</b> Again measure and cluster using the mean values. This way it may not lead to good clustering.</li><br>
	<li><b>[STEP 8]:</b> But to assess the quality of clustering by adding up the variation within each cluster.</li><br>
	<li><b>[STEP 9]:</b> So it beign the machine it cannot judge the best initial points, we have to do above said complete
procedure by choosing dierent initial points and keep the mean and variance. This will repeat for all the
points in group.</li><br>
	<li><b>[STEP 10]:</b> Once after all the clustering is compared we can understand the variance and see which is the
best clustering out of all the samples.</li><br>
	<li><b>[STEP 11]:</b> How do we know best K value ?</li>
<p>Answer : one possible way is to start with K = 1 and try dierent values and check the total variation
against the next number. As you increase you may see improvement. But after some time you may end up
not seeing benet in higher values. When the K reaches max number of points we get the variation 0. If we
plot all these iterations of variance and K We can easily notice after what value of K it will not impact any
more.</p><br>
	<li><b>[STEP 12]:</b> How do we know best points to choose to reduce the number of iterations in algorithm.</li><br>
	<li><b>[STEP 13]:</b> What happens if we have the 2-D points instead of point on single line.</li><br>
<p>Answer: We have to consider the euclidean distance this is same pythogarus distance of the selected point
to new point.
<img src="http://latex.codecogs.com/gif.latex?$\sqrt{x^{2} + y^{2}}$" border="0"/><br></p>
<li><b>[STEP 14]:</b> What is my data is a heat map?</li>
<p>Answer: If we have 2 samples we can plot the same on X & Y map. If we have more than 2 dimensional
data.<br>
ex1: 3 D data, euclidean distance is <img src="http://latex.codecogs.com/gif.latex?$\sqrt{x^{2} + y^{2} + z^{2} }$" border="0"/><br>
ex2: 4 D data, euclidean distance is <img src="http://latex.codecogs.com/gif.latex?$\sqrt{x^{2} + y^{2} + z^{2} + a^{2}}$" border="0"/><br>
Some more important information</p><br>

</ul>

<h3>Affinity Propogation</h3>
<h3>Mean-shift</h3>
<h3>Spectral Clustering</h3>
<h3>Ward Hierarchial Clustering</h3>
<p>Hierarchial clustering is often associated with heat maps. Heat maps are where columns represent dierent
samples and the rows represent measurements from different genes. Red means high expressino of gene and
blue or purple means lower expression of gene.</p>
<h3>Agglomerative clustering</h3>
<h3>DBSCAN</h3>
<h3>Gaussian Mixtures</h3>
<h3>Brich</h3>


<h2> Usecase</h2>
<h2> Example</h2>
<h2> Code</h2>
<h2> Links </h2>
<p>Following section has all the links from which the above content is been understood and presnted.</p>
<ul>
<li><a href="https://towardsdatascience.com/deep-learning-with-python-703e26853820">1. https://towardsdatascience.com/deep-learning-with-python-703e26853820</a></li>
</ul>
<h2> Conclusion </h2>
</div>
{% endblock content %}