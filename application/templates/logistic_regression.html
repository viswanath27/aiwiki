{% extends "layout.html" %}
{% block content %}
<div>
<h1> Logistic Regression Analysis </h1>
<p>This section explains the Linear Discriminant Analysis of the machine learning and its uses and its examples
Little bit about the problem statement of where Linear Discriminant Analysis is useful.</p>
<p>Suppose there is this new cancer drug
	<ul>
<li>1) It works great for some people.</li>
<li>2) But for some people it makes it worse.</li>
</ul>
</p>


<h3>Use-cases</h3>
<p>Before getting into Logistic Regression understanding following are some of the use cases where this model
is used.</p>
<ul>
<li>1. Classifying banking customers on whether or not they will default on a loan.</li>
<li>2. Predicting whether or not a skin lesion is benign or malignant based on some criteria.</li>
<li>3. Categorizing the mail as spam or not</li>
</ul>
<p>Basically this form or analysis will lead to a results of YES or NO.</p>
<h3>Algorithm</h3>
<p>This section explains the Logistic Regression model of the machine learning and its uses and its examples.
This the best method to choose if we are going for the binary classication. Logistic function is nothing but
the sigmod function. It is shown by following equation</p>


<p>
  $$S(x) = {1 \over {1 + e^{-x}}}.$$
</p>

<p>Logistic Regression function is as shown below.</p>

<p>
  $$y = {e^{B0+B1x} \over {1 + e^{B0+B1x}}}.$$
</p>

<p>Above is the Logistic Regression function</p>

<ul>
	<li>1) (x) : is input</li>
	<li>2) (B) : is the weights or coefficient values</li>
	<li>3) (y) : Predicted output values</li>
	<li>4) (B0) : is the bias or intercept</li>
	<li>5) (B1) : is the coefficient for the single input value (x)</li>
</ul>
The main difference from linear regression is that output value is being modeled as the binary values (0 or
1) rather numeric value.
<p>Logistic regression is a liner model, but the predictions are using Logistic function, resulting in predictions
being non-linear combination of inputs.This generally happens in Linear regression.</p>

<p>
  $$P(x) = {e^{B0+B1x} \over {1 + e^{B0+B1x}}}.$$
</p>

By applying natural logarithm (ln) on both sides above equation will lead to below format.

<p>
  $$ln({P(x) \over 1 - P(x)}) = B0 + B1x.$$
</p>


<p>Coefficients of the logistic regression algorithm must be estimated from your training data. Maximum-
likelihood estimation is the algorithm which is used to caclulate these coefficients. This is implemented using
Quasi-newton method which is numeric optimization algorithm. This can also be implemented using simpler Gradient descent algorithm</p>

<h3>Maximum-likelihood estimation</h3>
<p>In statistics Maximum likelihood estimation (MLE) is a method of estimating the parameters of a statistical
model. The method obtains the parameter estimates by finding the parameter values that maximize the
liklihood function.</p>

<h3>Quasi-newton method</h3>
<p>Quasi-Newton methods are the methods used to find either zeros or local maxima and minima of functions,
as an alternative to Newton's method. They can be used if the 'Jacobian' or 'Hessian' is unavailable or too
expensive to compute at every iteration.</p>
<h3>Gradient descent algorithm</h3>
<p>Its a first order iterative optimization algorithm for finding the minimum of a function. To find the local
minimum of function using the gradient descent one takes steps proportional to the negative of the gradient
of the function at the current point.</p>
<h3>Physical significance</h3>
<p>F(x) is the multi variable function is defined and differentiable in a neighbourhood of a point a, then F(x)
decreases fast if one goes from a in the direction of the negative gradient of F at a,rF(a). it follows that, if</p>

<h3>Examples of its usage</h3>

<pre><code class="html">
x = 5;
y = 6;
z = x + y;
</code></pre>

<h3>Behaviour</h3>
This function generally shows its behvaiour as shown below:<br>

<img src="{{url_for('static', filename='Sigmod.png')}}" alt="" align="middle" width="700" height="400"><br><br>

</div>
{% endblock content %}